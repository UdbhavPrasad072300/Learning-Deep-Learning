{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Deep Learning: Modern Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Deep Feedforward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also called multilayer perceptrons (MLPs) <br>\n",
    "The goal of this network is to approximate some function <br>\n",
    "A chain of functions are used for this function: $f(x) = f^{(3)}(f^{(2)}(f^{(1)}((x))))$ <br>\n",
    "^ f1 is first layer ^ f2 is second layer ^ f3 is third layer and so on <br>\n",
    "^ Number of layers accounts for <b>depth</b> of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality of these hidden layers determines the width of the model <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a non-linear function rather than a linear function as it has limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want $f$ to be as close to $f^*$; $θ$ is trainable parameters <br> \n",
    "The Mean Squared Loss (MSE) Function: <br>\n",
    "J(θ) = $\\frac{1}{4}\\sum_{x∈X}^{} (f^∗(x) − f(x; θ))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If function is linear: $f(x; w, b) = x^Tw + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU: $g(z) = max(0, z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How a Model processes Batches of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let input be $X$, a design matrix where each row is an input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You multiply the Input Matrix by the Weight Matrix then add the bias vector (LINEAR FUNCTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2: Gradient Based Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-linearity of a neural network causes most interesting functions to become non-convex. <br>\n",
    "Stochastic gradient descent applied to nonconvex loss functions has no such convergence guarantee and is sensitive to the values of the initial parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1: Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative Log-likelihood: <br>\n",
    "$J(θ) = −E_{x,y∼pˆ_{data}} log p_{model}(y | x)$ <br>\n",
    "$p_{model}(y | x) = N (y; f(x; θ), I)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Entropy Cost Function is more popular than mean squared error or mean absolute error because these two often yeild to poor results from gradient-based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2: Output Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2.1: Linear Units for Gaussian Output Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2.2: Sigmoid Units for Berrnoulli Output Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$yˆ = σ(w^Th + b)$ <br>\n",
    "where, σ is the logistic sigmoid function (converts to probability for us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2.3: Softmax Units for Multinoulli Output Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability distribution over a discrete variable with n possible values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2.4: Other Output Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks with Gaussian mixtures as their output are often called mixture density networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Hidden Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU is a good choice; also, the ReLU function is not differentiable at 0 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.1 Rectified Linear Units and Their Generalizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU use activation function $g(z) = max(0, z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute value rectification $g(z) = |z|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametric ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rectified linear units and all these generalizations of them are based on the\n",
    "principle that models are easier to optimize if their behavior is closer to linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2: Logistic Sigmoid and Hyperbolic Tangent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Sigmoid Activation Function: $g(z) =  σ(z)$ <br>\n",
    "Hyperbolic Tangent Activation Function: $g(z) = tanh(z)$ <br>\n",
    "These two functions are closely related because $tanh(z) = 2σ(2z)-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.3: Other Hidden Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to have no activation function at all (Identity Function) <br>\n",
    "A few other reasonably common hidden types include: <br>\n",
    "1) Radial basis function (RBF) unit <br>\n",
    "2) Softplus <br>\n",
    "3) Hard tanh <hr>\n",
    "<b>NOTE FROM TEXTBOOK: Hidden unit design remains an active area of research, and many useful hidden\n",
    "unit types remain to be discovered </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4: Architecture Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideal network architecture for a task must be found via experimentation guided by monitoring the validation set error <br><br>\n",
    "Neural Networks can be described through depth of a network and width of each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.1: Universal Approximation Properties and Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.2: Other Architectural Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5: Back-Propagation and Other Differentiation Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backpropagation is propagating backwards in the neural network to calculate the gradients. The\n",
    "back-propagation algorithm does so using a simple and inexpensive procedure. <br>\n",
    "The backpropagation algorithm is often simply called backprop <hr>\n",
    "Forward Propagation: From input $x$ we produce an output $y^$ (y hat) from the MLP <br>\n",
    "Backward Propagation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.1 Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A node in the graph is to indicate a variable (it can be a scalar, vector, matrix, tensor or anything else)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.2: Chain Rule of Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.3: Recursively Applying the Chain Rule to Obtain Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.4: Back-Propagation Computation in Fully Connected MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.5: Symbol-to-Symbol Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.6: General Back-Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.7: Example: Back-Propagation for MLP Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.8: Complications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-world implementations of back-propagation also need to handle various data types, such as 32-bit floating point, 64-bit floating point, and integer values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.9: Differentiation outside the Deep Learning Community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field of automatic differentiation is concerned with how to compute derivatives algorithmically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.10: Higher-Order Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6: Historical Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Feedforward networks can be seen as efficient nonlinear function approximators\n",
    "based on using gradient descent to minimize the error in a function approximation.\n",
    "From this point of view, the modern feedforward network is the culmination of\n",
    "centuries of progress on the general function approximation task.</b><br>\n",
    "One of these algorithmic changes was the replacement of mean squared error\n",
    "with the cross-entropy family of loss functions. Mean squared error was popular in\n",
    "the 1980s and 1990s but was gradually replaced by cross-entropy losses and the\n",
    "principle of maximum likelihood as ideas spread between the statistics community\n",
    "and the machine learning community."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
