{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14: Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is a neural network that is trained to attempt to copy its input to its output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view it as having two parts: the <b>Encoder</b> and the <b>Decoder</b><br>\n",
    "Encoder: $h = f(x)$<br>\n",
    "Decoder: $r = g(h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Traditionally, autoencoders were used for dimensionality reduction or\n",
    "feature learning. Recently, theoretical connections between autoencoders and\n",
    "latent variable models have brought autoencoders to the forefront of generative\n",
    "modeling</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1: Undercomplete Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder whose code dimension is less than the input dimension is called undercomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder trained to perform the copying task can fail to learn anything useful about\n",
    "the dataset if the capacity of the autoencoder is allowed to become too great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2: Regularized Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regularized autoencoder can be nonlinear and overcomplete but still learn something useful about the data distribution, even if the model capacity is great enough to learn a trivial identity function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their encodings are naturally useful because the models were trained to approximately maximize\n",
    "the probability of the training data rather than to copy the input to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2.1: Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sparse autoencoder is simply an autoencoder whose training criterion involves a\n",
    "sparsity penalty Ω(h) on the code layer h, in addition to the reconstruction error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse autoencoders are typically used to learn features for another task, such\n",
    "as classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the penalty Ω(h) simply as a regularizer term added to\n",
    "a feedforward network whose primary task is to copy the input to the output\n",
    "(unsupervised learning objective) and possibly also perform some supervised task\n",
    "(with a supervised learning objective) that depends on these sparse features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than thinking of the sparsity penalty as a regularizer for the copying\n",
    "task, we can think of the entire sparse autoencoder framework as approximating maximum likelihood training of a generative model that has latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2.2: Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than adding a penalty Ω to the cost function, we can obtain an autoencoder\n",
    "that learns something useful by changing the reconstruction error term of the cost\n",
    "function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally, autoencoders minimize some function: $L(x, g(f(x)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A denoising autoencoder (DAE) instead minimizes: $L(x, g(f(x˜)))$ <br>\n",
    "where x˜ is a copy of x that has been corrupted by some form of noise. Denoising\n",
    "autoencoders must therefore undo this corruption rather than simply copying their\n",
    "input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2.3: Regularizing by Penalizing Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look up: contractive autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3: Representational Power, Layer Size and Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deep autoencoder, with at least one additional hidden layer inside the encoder itself, can approximate any mapping from input to code arbitrarily well, given enough hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depth can exponentially reduce the computational cost of representing some\n",
    "functions. Depth can also exponentially decrease the amount of training data\n",
    "needed to learn some functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.4: Stochastic Encoders and Decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.5: Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denoising autoencoder (DAE) is an autoencoder that receives a corrupted\n",
    "data point as input and is trained to predict the original, uncorrupted data point\n",
    "as its output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.5.1: Estimating the Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.5.1.1: Historical Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.6: Learning Manifolds with Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like many other machine learning algorithms, autoencoders exploit the idea\n",
    "that data concentrates around a low-dimensional manifold or a small set of such\n",
    "manifolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.7: Contractive Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoising autoencoders make the reconstruction function resist small but\n",
    "finite-sized perturbations of the input, while contractive autoencoders make the\n",
    "feature extraction function resist infinitesimal perturbations of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.8: Predictive Sparse Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive sparse decomposition (PSD) is a model that is a hybrid of sparse\n",
    "coding and parametric autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.9: Applications of Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower-dimensional representations can improve performance on many tasks,\n",
    "such as classification; Models of smaller spaces consume less memory and runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One task that benefits even more than usual from dimensionality reduction is\n",
    "information retrieval, the task of finding entries in a database that resemble a\n",
    "query entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
