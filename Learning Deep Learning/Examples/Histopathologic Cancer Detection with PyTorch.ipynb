{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histopathologic Cancer Detection with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSet: https://www.kaggle.com/c/histopathologic-cancer-detection/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import utils\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(\"train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df[\"label\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allIDs = labels_df.loc[labels_df[\"label\"]][\"id\"].values\n",
    "allIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = True\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "nrows, ncols= 3, 3\n",
    "\n",
    "for i,id_ in enumerate(allIDs[:nrows * ncols]):\n",
    "    full_filenames = os.path.join(\"train\", id_ +'.tif')\n",
    "    \n",
    "    # load image\n",
    "    img = Image.open(full_filenames)\n",
    "    \n",
    "    # draw a 32*32 rectangle\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.rectangle(((32, 32), (64, 64)),outline=\"green\")\n",
    "    plt.subplot(nrows, ncols, i+1)\n",
    "    if color is True:\n",
    "        plt.imshow(np.array(img))\n",
    "    else:\n",
    "        plt.imshow(np.array(img)[:,:,0],cmap=\"gray\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"image shape:\", np.array(img).shape)\n",
    "print(\"pixel values range from %s to %s\" %(np.min(img), np.max(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class histoCancerDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform,data_type=\"train\"):\n",
    "        # path to images\n",
    "        path2data=os.path.join(data_dir,data_type)\n",
    "        # get a list of images\n",
    "        filenames = os.listdir(path2data)\n",
    "        # get the full path to images\n",
    "        self.full_filenames = [os.path.join(path2data, f) for f in filenames]\n",
    "        # labels are in a csv file named train_labels.csv\n",
    "        csv_filename=data_type+\"_labels.csv\"\n",
    "        path2csvLabels=os.path.join(data_dir,csv_filename)\n",
    "        labels_df=pd.read_csv(path2csvLabels)\n",
    "        # set data frame index to id\n",
    "        labels_df.set_index(\"id\", inplace=True)\n",
    "        # obtain labels from data frame\n",
    "        self.labels = [labels_df.loc[filename[:-4]].values[0] for\n",
    "        filename in filenames]\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        # return size of dataset\n",
    "        return len(self.full_filenames)\n",
    "    def __getitem__(self, idx):\n",
    "        # open image, apply transforms and return with label\n",
    "        image = Image.open(self.full_filenames[idx]) # PIL image\n",
    "        image = self.transform(image)\n",
    "        return image, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformer = transforms.Compose([transforms.ToTensor()])\n",
    "histo_dataset = histoCancerDataset(\"./\", data_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = histo_dataset[10]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_histo = len(histo_dataset)\n",
    "len_histo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = int(0.8 * len_histo)\n",
    "len_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_val = len_histo - len_train\n",
    "len_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = random_split(histo_dataset, [len_train,len_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train dataset length:\", len(train_ds))\n",
    "print(\"validation dataset length:\", len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_ds:\n",
    "    print(x.shape,y)\n",
    "    break\n",
    "for x,y in val_ds:\n",
    "    print(x.shape,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img,y,color=False):\n",
    "    # convert tensor to numpy array\n",
    "    npimg = img.numpy()\n",
    "    # Convert to H*W*C shape\n",
    "    npimg_tr=np.transpose(npimg, (1,2,0))\n",
    "    if color==False:\n",
    "        npimg_tr=npimg_tr[:,:,0]\n",
    "        plt.imshow(npimg_tr,interpolation='nearest',cmap=\"gray\")\n",
    "    else:\n",
    "        # display images\n",
    "        plt.imshow(npimg_tr,interpolation='nearest')\n",
    "    plt.title(\"label: \"+str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for x,y in train_ds:\n",
    "    show(x, y, color=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomResizedCrop(96, scale=(0.8,1.0), ratio=(1.0,1.0)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transformer = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite the transform functions\n",
    "train_ds.transform=train_transformer\n",
    "val_ds.transform=val_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a batch from training data\n",
    "for x, y in train_dl:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_dl:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, out):\n",
    "    return np.sum(out==labels)/float(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findConv2dOutShape(H_in, W_in, conv, pool=2):\n",
    "    # get conv arguments\n",
    "    kernel_size = conv.kernel_size\n",
    "    stride = conv.stride\n",
    "    padding = conv.padding\n",
    "    dilation = conv.dilation\n",
    "    \n",
    "    # Ref: https://pytorch.org/docs/stable/nn.html\n",
    "    H_out = np.floor((H_in+2*padding[0] - dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n",
    "    W_out = np.floor((W_in+2*padding[1] - dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n",
    "    if pool:\n",
    "        H_out /= pool\n",
    "        W_out /= pool\n",
    "    return int(H_out), int(W_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        C_in, H_in, W_in = params[\"input_shape\"]\n",
    "        init_f = params[\"initial_filters\"]\n",
    "        num_fc1 = params[\"num_fc1\"]\n",
    "        num_classes = params[\"num_classes\"]\n",
    "        self.dropout_rate = params[\"dropout_rate\"]\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(C_in, init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(H_in, W_in, self.conv1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h, w, self.conv2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h, w, self.conv3)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(4*init_f, 8*init_f, kernel_size=3)\n",
    "        h,w=findConv2dOutShape(h, w, self.conv4)\n",
    "        \n",
    "        # compute the flatten size\n",
    "        self.num_flatten= h * w * 8 * init_f\n",
    "        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n",
    "        self.fc2 = nn.Linear(num_fc1, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, self.num_flatten)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, self.dropout_rate, training= self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_model={\n",
    "    \"input_shape\": (3,96,96),\n",
    "    \"initial_filters\": 8,\n",
    "    \"num_fc1\": 100,\n",
    "    \"dropout_rate\": 0.25,\n",
    "    \"num_classes\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(params_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(3, 96, 96), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.NLLLoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get learning rate\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    lr_scheduler.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_batch(output, target):\n",
    "    # get output class\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    # compare output class with target class\n",
    "    corrects=pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    loss = loss_func(output, target)\n",
    "    with torch.no_grad():\n",
    "        metric_b = metrics_batch(output,target)\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return loss.item(), metric_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_epoch(model,loss_func,dataset_dl,sanity_check=False,opt=None):\n",
    "    running_loss = 0.0\n",
    "    running_metric=0.0\n",
    "    len_data=len(dataset_dl.dataset)\n",
    "    for xb, yb in dataset_dl:\n",
    "        # move batch to device\n",
    "        xb=xb.to(device)\n",
    "        yb=yb.to(device)\n",
    "        # get model output\n",
    "        output=model(xb)\n",
    "        # get loss per batch\n",
    "        loss_b,metric_b=loss_batch(loss_func, output, yb, opt)\n",
    "        # update running loss\n",
    "        running_loss+=loss_b\n",
    "        # update running metric\n",
    "        if metric_b is not None:\n",
    "            running_metric+=metric_b\n",
    "        # break the loop in case of sanity check\n",
    "        if sanity_check is True:\n",
    "            break\n",
    "    # average loss value\n",
    "    loss=running_loss/float(len_data)\n",
    "    # average metric value\n",
    "    metric=running_metric/float(len_data)\n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(model, params):\n",
    "    # extract model parameters\n",
    "    num_epochs=params[\"num_epochs\"]\n",
    "    loss_func=params[\"loss_func\"]\n",
    "    opt=params[\"optimizer\"]\n",
    "    train_dl=params[\"train_dl\"]\n",
    "    val_dl=params[\"val_dl\"]\n",
    "    sanity_check=params[\"sanity_check\"]\n",
    "    lr_scheduler=params[\"lr_scheduler\"]\n",
    "    path2weights=params[\"path2weights\"]\n",
    "    # history of loss values in each epoch\n",
    "    loss_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "    # history of metric values in each epoch\n",
    "    metric_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "    # a deep copy of weights for the best performing model\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    # initialize best loss to a large value\n",
    "    best_loss=float('inf')\n",
    "    # main loop\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # get current learning rate\n",
    "        current_lr=get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs - 1, current_lr))\n",
    "        # train model on training dataset\n",
    "        model.train()\n",
    "        train_loss, train_metric = loss_epoch(model,loss_func,train_dl,sanity_check,opt)\n",
    "        # collect loss and metric for training dataset\n",
    "        loss_history[\"train\"].append(train_loss)\n",
    "        metric_history[\"train\"].append(train_metric)\n",
    "        # evaluate model on validation dataset\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric = loss_epoch(model,loss_func,val_dl,sanity_check)\n",
    "        # collect loss and metric for validation dataset\n",
    "        loss_history[\"val\"].append(val_loss)\n",
    "        metric_history[\"val\"].append(val_metric)\n",
    "        # store best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            # store weights into a local file\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print(\"Copied best model weights!\")\n",
    "        # learning rate schedule\n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            print(\"Loading best model weights!\")\n",
    "            model.load_state_dict(best_model_wts)\n",
    "        print(\"train loss: %.6f, dev loss: %.6f, accuracy: %.2f\"%(train_loss,val_loss,100*val_metric))\n",
    "        print(\"-\"*10)\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_train={\n",
    "    \"num_epochs\": 100,\n",
    "    \"optimizer\": opt,\n",
    "    \"loss_func\": loss_func,\n",
    "    \"train_dl\": train_dl,\n",
    "    \"val_dl\": val_dl,\n",
    "    \"sanity_check\": False,\n",
    "    \"lr_scheduler\": lr_scheduler,\n",
    "    \"path2weights\": \"./models/weights.pt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, loss_hist, metric_hist = train_val(model, params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation Progress\n",
    "num_epochs=params_train[\"num_epochs\"]\n",
    "# plot loss progress\n",
    "plt.title(\"Train-Val Loss\")\n",
    "plt.plot(range(1,num_epochs+1),loss_hist[\"train\"],label=\"train\")\n",
    "plt.plot(range(1,num_epochs+1),loss_hist[\"val\"],label=\"val\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plot accuracy progress\n",
    "plt.title(\"Train-Val Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),metric_hist[\"train\"],label=\"train\")\n",
    "plt.plot(range(1,num_epochs+1),metric_hist[\"val\"],label=\"val\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
