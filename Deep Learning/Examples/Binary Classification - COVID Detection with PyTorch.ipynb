{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification: COVID Detection in X-Rays with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import utils\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPIL = transforms.Compose([\n",
    "            transforms.ToPILImage()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidDatasetTrain(Dataset):\n",
    "    def __init__(self):\n",
    "        train_dir = \"xray_dataset_covid19/train/\"\n",
    "        \n",
    "        train_normal_dir = train_dir + \"NORMAL/\"\n",
    "        train_pneumonia_dir = train_dir + \"PNEUMONIA/\"\n",
    "        \n",
    "        train_normal_fnames = os.listdir(train_normal_dir)\n",
    "        train_pneumonia_fnames = os.listdir(train_pneumonia_dir)\n",
    "        \n",
    "        self.train_dataset = [[train_normal_dir + image, 0] for image in train_normal_fnames]\n",
    "        self.train_dataset = self.train_dataset + [[train_pneumonia_dir + image, 1] for image in train_pneumonia_fnames]\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize(1024),\n",
    "            transforms.CenterCrop(1024),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.train_dataset))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.train_dataset[idx]\n",
    "        image = Image.open(data[0])\n",
    "        image = self.transform(image)\n",
    "        return(image, data[1], idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidDatasetTest(Dataset):\n",
    "    def __init__(self):\n",
    "        test_dir = \"xray_dataset_covid19/test/\"\n",
    "        \n",
    "        test_normal_dir = test_dir + \"NORMAL/\"\n",
    "        test_pneumonia_dir = test_dir + \"PNEUMONIA/\"\n",
    "        \n",
    "        test_normal_fnames = os.listdir(test_normal_dir)\n",
    "        test_pneumonia_fnames = os.listdir(test_pneumonia_dir)\n",
    "        \n",
    "        self.test_dataset = [[test_normal_dir + image, 0] for image in test_normal_fnames]\n",
    "        self.test_dataset = self.test_dataset + [[test_pneumonia_dir + image, 1] for image in test_pneumonia_fnames]\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize(1024),\n",
    "            transforms.CenterCrop(1024),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.test_dataset))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.test_dataset[idx]\n",
    "        image = Image.open(data[0])\n",
    "        image = self.transform(image)\n",
    "        return(image, data[1], idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = CovidDatasetTrain()\n",
    "covid_trainloader = DataLoader(train_df, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = CovidDatasetTest()\n",
    "covid_testloader = DataLoader(test_df, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1024, 1024])\n",
      "tensor([[1],\n",
      "        [1]])\n",
      "torch.Size([2, 1])\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, label, _) in enumerate(covid_trainloader):\n",
    "    print(data.size())\n",
    "    label = label.unsqueeze(1)\n",
    "    print(label)\n",
    "    print(label.size())\n",
    "    break \n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1024, 1024])\n",
      "tensor([[1],\n",
      "        [1]])\n",
      "torch.Size([2, 1])\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, label, _) in enumerate(covid_testloader):\n",
    "    print(data.size())\n",
    "    label = label.unsqueeze(1)\n",
    "    print(label)\n",
    "    print(label.size())\n",
    "    break \n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2)\n",
    "        \n",
    "        self.num_flatten = 128 * 3 * 3\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.num_flatten, 100)\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "    \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = x.view(-1, self.num_flatten)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (fc1): Linear(in_features=1152, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = CNN()\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (fc1): Linear(in_features=1152, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 511, 511]             160\n",
      "            Conv2d-2         [-1, 32, 127, 127]           4,640\n",
      "            Conv2d-3           [-1, 64, 31, 31]          18,496\n",
      "            Conv2d-4            [-1, 128, 7, 7]          73,856\n",
      "            Linear-5                  [-1, 100]         115,300\n",
      "            Linear-6                    [-1, 1]             101\n",
      "================================================================\n",
      "Total params: 212,553\n",
      "Trainable params: 212,553\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 4.00\n",
      "Forward/backward pass size (MB): 36.33\n",
      "Params size (MB): 0.81\n",
      "Estimated Total Size (MB): 41.14\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Udbhav Prasad\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "summary(classifier, input_size=(1, 1024, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epoches, loss_hist):    \n",
    "    \n",
    "    for epoch in range(1, n_epoches+1):\n",
    "        model.train()\n",
    "    \n",
    "        epoch_train_loss = 0\n",
    "        epoch_test_loss = 0\n",
    "        \n",
    "        y_true_train = []\n",
    "        y_pred_train = []\n",
    "        \n",
    "        y_true_test = []\n",
    "        y_pred_test = []\n",
    "        \n",
    "        for batch_idx, (data, label, _) in enumerate(covid_trainloader):\n",
    "            data = data.to(device)\n",
    "            label = label.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "            preds = model(data)\n",
    "            \n",
    "            loss = loss_function(preds, label.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            y_pred_train.extend(preds.squeeze(1).detach().round().tolist())\n",
    "            y_true_train.extend(label.detach().round().tolist())\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "                \n",
    "            for batch_idx, (data, label, _) in enumerate(covid_testloader):\n",
    "                data = data.to(device)\n",
    "                label = label.type(torch.FloatTensor).to(device)\n",
    "                    \n",
    "                preds = model(data)\n",
    "                \n",
    "                val_loss = loss_function(preds, label.unsqueeze(1))\n",
    "                \n",
    "                y_pred_test.extend(preds.squeeze(1).detach().round().tolist())\n",
    "                y_true_test.extend(label.detach().round().tolist())\n",
    "                \n",
    "                epoch_test_loss += val_loss.item()\n",
    "        \n",
    "        #print(y_pred_test, \"<>\", y_true_test)\n",
    "        \n",
    "        epoch_train_loss = epoch_train_loss / len(covid_trainloader.dataset)\n",
    "        epoch_test_loss = epoch_test_loss / len(covid_testloader.dataset)\n",
    "        \n",
    "        loss_hist[\"train loss\"].append(epoch_train_loss)\n",
    "        loss_hist[\"test loss\"].append(epoch_test_loss)\n",
    "        \n",
    "        print(\"-------------------------------------------------\")\n",
    "        print(\"Epoch: {} Train mean loss: {:.8f}\".format(epoch, epoch_train_loss))\n",
    "        print(\"       {} Test  mean loss: {:.8f}\".format(epoch, epoch_test_loss))\n",
    "        print(\"       Train Accuracy: \", len([True for x, y in zip(y_pred_train, y_true_train) if x==y])/len(y_pred_train), \"==\", len([True for x, y in zip(y_pred_train, y_true_train) if x==y]), \"/\", len(y_pred_train))\n",
    "        print(\"       Test Accuracy: \", len([True for x, y in zip(y_pred_test, y_true_test) if x==y])/len(y_pred_test), \"==\", len([True for x, y in zip(y_pred_test, y_true_test) if x==y]), \"/\", len(y_pred_test))\n",
    "        print(\"-------------------------------------------------\")\n",
    "    return loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 1 Train mean loss: 0.34738674\n",
      "       1 Test  mean loss: 0.34716849\n",
      "       Train Accuracy:  0.5202702702702703 == 77 / 148\n",
      "       Test Accuracy:  0.5 == 20 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 2 Train mean loss: 0.34499240\n",
      "       2 Test  mean loss: 0.34474061\n",
      "       Train Accuracy:  0.5540540540540541 == 82 / 148\n",
      "       Test Accuracy:  0.45 == 18 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 3 Train mean loss: 0.33977035\n",
      "       3 Test  mean loss: 0.34762985\n",
      "       Train Accuracy:  0.6216216216216216 == 92 / 148\n",
      "       Test Accuracy:  0.55 == 22 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 4 Train mean loss: 0.25426642\n",
      "       4 Test  mean loss: 0.16357259\n",
      "       Train Accuracy:  0.7702702702702703 == 114 / 148\n",
      "       Test Accuracy:  0.8 == 32 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 5 Train mean loss: 0.21467999\n",
      "       5 Test  mean loss: 0.16802298\n",
      "       Train Accuracy:  0.7972972972972973 == 118 / 148\n",
      "       Test Accuracy:  0.775 == 31 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 6 Train mean loss: 0.14587320\n",
      "       6 Test  mean loss: 0.15328830\n",
      "       Train Accuracy:  0.8648648648648649 == 128 / 148\n",
      "       Test Accuracy:  0.875 == 35 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 7 Train mean loss: 0.14786644\n",
      "       7 Test  mean loss: 0.14773751\n",
      "       Train Accuracy:  0.8851351351351351 == 131 / 148\n",
      "       Test Accuracy:  0.9 == 36 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 8 Train mean loss: 0.12812696\n",
      "       8 Test  mean loss: 0.13206294\n",
      "       Train Accuracy:  0.9256756756756757 == 137 / 148\n",
      "       Test Accuracy:  0.9 == 36 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 9 Train mean loss: 0.09978446\n",
      "       9 Test  mean loss: 0.11995711\n",
      "       Train Accuracy:  0.9527027027027027 == 141 / 148\n",
      "       Test Accuracy:  0.875 == 35 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 10 Train mean loss: 0.09095805\n",
      "       10 Test  mean loss: 0.17605838\n",
      "       Train Accuracy:  0.918918918918919 == 136 / 148\n",
      "       Test Accuracy:  0.775 == 31 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 11 Train mean loss: 0.07359881\n",
      "       11 Test  mean loss: 0.14573572\n",
      "       Train Accuracy:  0.9459459459459459 == 140 / 148\n",
      "       Test Accuracy:  0.9 == 36 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 12 Train mean loss: 0.09509609\n",
      "       12 Test  mean loss: 0.23996710\n",
      "       Train Accuracy:  0.9391891891891891 == 139 / 148\n",
      "       Test Accuracy:  0.725 == 29 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 13 Train mean loss: 0.07205313\n",
      "       13 Test  mean loss: 0.05086288\n",
      "       Train Accuracy:  0.9594594594594594 == 142 / 148\n",
      "       Test Accuracy:  0.975 == 39 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 14 Train mean loss: 0.06466420\n",
      "       14 Test  mean loss: 0.10165619\n",
      "       Train Accuracy:  0.9527027027027027 == 141 / 148\n",
      "       Test Accuracy:  0.925 == 37 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 15 Train mean loss: 0.04433573\n",
      "       15 Test  mean loss: 0.15764523\n",
      "       Train Accuracy:  0.9594594594594594 == 142 / 148\n",
      "       Test Accuracy:  0.925 == 37 / 40\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train loss': [0.3473867447795095,\n",
       "  0.34499240203483683,\n",
       "  0.3397703543808815,\n",
       "  0.25426642141128714,\n",
       "  0.2146799931856426,\n",
       "  0.14587319618393038,\n",
       "  0.1478664415850732,\n",
       "  0.12812695568464808,\n",
       "  0.09978445764077273,\n",
       "  0.09095804646180163,\n",
       "  0.0735988136256052,\n",
       "  0.09509608777866438,\n",
       "  0.07205312608594623,\n",
       "  0.06466420438369529,\n",
       "  0.04433572535934808],\n",
       " 'test loss': [0.34716848731040956,\n",
       "  0.3447406068444252,\n",
       "  0.34762985333800317,\n",
       "  0.16357258693315088,\n",
       "  0.16802298189140857,\n",
       "  0.1532883032137761,\n",
       "  0.14773751349421219,\n",
       "  0.13206294309784425,\n",
       "  0.11995710904648149,\n",
       "  0.1760583763199975,\n",
       "  0.14573571643209107,\n",
       "  0.23996709922212175,\n",
       "  0.0508628816336568,\n",
       "  0.10165619306062582,\n",
       "  0.15764522823228616]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_hist = {}\n",
    "loss_hist[\"train loss\"] = []\n",
    "loss_hist[\"test loss\"] = []\n",
    "\n",
    "NUM_EPOCHES = 15\n",
    "\n",
    "loss_hist = train(classifier, NUM_EPOCHES, loss_hist)\n",
    "loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
